{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.backend import clear_session\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import optuna\n",
    "\n",
    "import psycopg2\n",
    "from config import *\n",
    "\n",
    "import utm\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(lon,lat,estaciones):\n",
    "    distancias_list=[]\n",
    "    cercano=0\n",
    "    closest=10000000\n",
    "    for i in range(len(estaciones)):\n",
    "        dist=(estaciones['longitude'].iloc[i]-lon)**2+(estaciones['latitude'].iloc[i]-lat)**2\n",
    "        distancias_list.append(dist)\n",
    "        if(dist<closest):\n",
    "            closest=dist\n",
    "            cercano=estaciones['nombrecorto'].iloc[i]\n",
    "    estaciones['distancias']=distancias_list\n",
    "    return cercano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conexion = psycopg2.connect(database=db_database, \n",
    "                                user=db_user, \n",
    "                                password=db_password, \n",
    "                                host=db_host, \n",
    "                                port=db_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''select date, variedad, min(phenologystageid) as phenologystageid, codigocatastro as codigo \n",
    "from redfara.redfara_fenologia\n",
    "where especie='VIÑEDO VINIFICACION'\n",
    "group by date, variedad, codigo, phenologystageid;'''\n",
    "phenological_data = pd.read_sql_query(query, con=conexion).drop_duplicates()\n",
    "conexion.commit()\n",
    "phenological_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='select * from cadastral.parcelas where codigo in '+ str(list(phenological_data.codigo.unique())).replace('[','(').replace(']',')') + ';'\n",
    "cadastral_data = pd.read_sql_query(query, con=conexion)\n",
    "conexion.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadastral_data['longitude']=cadastral_data['coordenadas_epsgwgs84'].apply(lambda x:float(x[0]))\n",
    "cadastral_data['latitude']=cadastral_data['coordenadas_epsgwgs84'].apply(lambda x:float(x[1]))\n",
    "cadastral_data=cadastral_data[['codigo', 'longitude','latitude','altitud']].drop_duplicates()\n",
    "cadastral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='select * from public.estara;'\n",
    "stations = pd.read_sql_query(query, con=conexion).drop_duplicates()\n",
    "conexion.commit()\n",
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations['longitude']=stations.longitud.str.split(\"'|º\").apply(lambda x:int(x[0])+int(x[1])/60+int(x[2])/3600000)\n",
    "stations['longitude']=stations['longitude']*(stations.longitud.str.contains('E')*2-1)\n",
    "stations['latitude']=stations.latitud.str.split(\"'|º\").apply(lambda x:int(x[0])+int(x[1])/60+int(x[2])/3600000)\n",
    "stations['latitude']=stations['latitude']*(stations.latitud.str.contains('N')*2-1)\n",
    "stations=stations[['nombrecorto','longitude','latitude', 'altitud']]\n",
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cercanias=[]\n",
    "for i in range(len(cadastral_data)):\n",
    "    cercanias.append(get_closest(cadastral_data.iloc[i].longitude,cadastral_data.iloc[i].latitude,stations))\n",
    "cadastral_data['closest']=cercanias\n",
    "cadastral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_stations = stations[stations.nombrecorto.isin(cadastral_data.closest.unique())]\n",
    "closest_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_year=phenological_data.date.dt.year.min()\n",
    "lowest_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='select * from public.meteorological_data WHERE anio >= ' + str(lowest_year) + ' AND estacion in ' + str(list(closest_stations.nombrecorto.unique())).replace('[','(').replace(']',')') + ';'\n",
    "meteorological_data = pd.read_sql_query(query, con=conexion).drop_duplicates()\n",
    "conexion.commit()\n",
    "meteorological_data = meteorological_data.drop(['ubi', 'season', 'hourFrac_sum'], axis=1)\n",
    "variables_diarias_min=['tmed_min', 'rad_min']\n",
    "variables_diarias_max=['tmed_max', 'rad_max']\n",
    "variables_diarias_mean=['tmed_mean', 'rad_mean', 'wind_N', 'wind_NE', 'wind_E','wind_SE', 'wind_S', 'wind_SW', \n",
    "                        'wind_W', 'wind_NW']\n",
    "variables_semanales=['gdd_4.5_t0_Tbase_sum',\n",
    "       'gdd_4.5_t0_TbaseMax_sum', 'gdd_4.5_1_Tbase_sum',\n",
    "       'gdd_4.5_1_TbaseMax_sum', 'gdd_4.5_2_Tbase_sum',\n",
    "       'gdd_4.5_2_TbaseMax_sum', 'gdd_10.0_t0_Tbase_sum',\n",
    "       'gdd_10.0_t0_TbaseMax_sum', 'gdd_10.0_1_Tbase_sum',\n",
    "       'gdd_10.0_1_TbaseMax_sum', 'gdd_10.0_2_Tbase_sum',\n",
    "       'gdd_10.0_2_TbaseMax_sum', 'chillingDD_7.0_t0_Tbase_sum',\n",
    "       'chillingDD_7.0_t0_Tbasemin_sum', 'chillingDD_7.0_t0_Utah_sum',\n",
    "       'chillingDD_7.0_1_Tbase_sum', 'chillingDD_7.0_1_Tbasemin_sum',\n",
    "       'chillingDD_7.0_1_Utah_sum', 'chillingDD_7.0_2_Tbase_sum',\n",
    "       'chillingDD_7.0_2_Tbasemin_sum', 'chillingDD_7.0_2_Utah_sum', 'rad_sum',\n",
    "       'precip_sum', 'winkler_4.5_Tbase', 'winkler_4.5_TbaseMax',\n",
    "       'winkler_10.0_Tbase', 'winkler_10.0_TbaseMax',\n",
    "       'gdd_4.5_t0_Tbase_sum_Cumm', 'gdd_4.5_t0_TbaseMax_sum_Cumm',\n",
    "       'gdd_4.5_1_Tbase_sum_Cumm', 'gdd_4.5_1_TbaseMax_sum_Cumm',\n",
    "       'gdd_4.5_2_Tbase_sum_Cumm', 'gdd_4.5_2_TbaseMax_sum_Cumm',\n",
    "       'gdd_10.0_t0_Tbase_sum_Cumm', 'gdd_10.0_t0_TbaseMax_sum_Cumm',\n",
    "       'gdd_10.0_1_Tbase_sum_Cumm', 'gdd_10.0_1_TbaseMax_sum_Cumm',\n",
    "       'gdd_10.0_2_Tbase_sum_Cumm', 'gdd_10.0_2_TbaseMax_sum_Cumm',\n",
    "       'chillingDD_7.0_t0_Tbase_sum_Cumm',\n",
    "       'chillingDD_7.0_t0_Tbasemin_sum_Cumm',\n",
    "       'chillingDD_7.0_t0_Utah_sum_Cumm', 'chillingDD_7.0_1_Tbase_sum_Cumm',\n",
    "       'chillingDD_7.0_1_Tbasemin_sum_Cumm', 'chillingDD_7.0_1_Utah_sum_Cumm',\n",
    "       'chillingDD_7.0_2_Tbase_sum_Cumm', 'chillingDD_7.0_2_Tbasemin_sum_Cumm',\n",
    "       'chillingDD_7.0_2_Utah_sum_Cumm', 'rad__t0__Cumm', 'rad__1__Cumm',\n",
    "       'rad__2__Cumm', 'precip__t0__Cumm', 'precip__1__Cumm',\n",
    "       'precip__2__Cumm', 'winkler_4.5_t0_Tbase_Cumm',\n",
    "       'winkler_4.5_t0_TbaseMax_Cumm', 'winkler_4.5_1_Tbase_Cumm',\n",
    "       'winkler_4.5_1_TbaseMax_Cumm', 'winkler_4.5_2_Tbase_Cumm',\n",
    "       'winkler_4.5_2_TbaseMax_Cumm', 'winkler_10.0_t0_Tbase_Cumm',\n",
    "       'winkler_10.0_t0_TbaseMax_Cumm', 'winkler_10.0_1_Tbase_Cumm',\n",
    "       'winkler_10.0_1_TbaseMax_Cumm', 'winkler_10.0_2_Tbase_Cumm',\n",
    "       'winkler_10.0_2_TbaseMax_Cumm']\n",
    "meteorological_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorological_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meteorological_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6e3e87e6d497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mn_dias_atras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_dias_alante\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mestacion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmeteorological_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'estacion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdatos_est\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeteorological_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmeteorological_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestacion\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mestacion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fecha'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fecha'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdatos_meteo_buenos_est_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatos_est\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'estacion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meteorological_data' is not defined"
     ]
    }
   ],
   "source": [
    "datos_meteo_buenos_list=[]\n",
    "n_dias_atras=14\n",
    "n_dias_alante=5\n",
    "for estacion in meteorological_data['estacion'].unique():\n",
    "    datos_est=meteorological_data[meteorological_data.estacion==estacion].sort_values('fecha').set_index('fecha')\n",
    "    datos_meteo_buenos_est_list=[datos_est[['estacion']]]\n",
    "    for var in variables_diarias_min:\n",
    "        datos_var_est=datos_est[[var]].resample('1D').min()\n",
    "        for i in range(1,n_dias_atras):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_atras\"]=datos_var_est[var].resample('1D').min().shift(i)\n",
    "        for i in range(1,n_dias_alante):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_adelante\"]=datos_var_est[var].resample('1D').min().shift(-i)\n",
    "        datos_meteo_buenos_est_list.append(datos_var_est)\n",
    "    for var in variables_diarias_max:\n",
    "        datos_var_est=datos_est[[var]].resample('1D').max()\n",
    "        for i in range(1,n_dias_atras):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_atras\"]=datos_var_est[var].resample('1D').max().shift(i)\n",
    "        for i in range(1,n_dias_alante):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_adelante\"]=datos_var_est[var].resample('1D').max().shift(-i)\n",
    "        datos_meteo_buenos_est_list.append(datos_var_est)\n",
    "    for var in variables_diarias_mean:\n",
    "        datos_var_est=datos_est[[var]].resample('1D').max()\n",
    "        for i in range(1,n_dias_atras):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_atras\"]=datos_var_est[var].resample('1D').max().shift(i)\n",
    "        for i in range(1,n_dias_alante):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_adelante\"]=datos_var_est[var].resample('1D').max().shift(-i)\n",
    "        datos_meteo_buenos_est_list.append(datos_var_est)\n",
    "    for var in variables_semanales:\n",
    "        datos_var_est=datos_est[[var]].resample('1D').max()\n",
    "        for i in range(1,1+n_dias_atras//7):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_semanas_atras\"]=datos_var_est[var].resample('1D').ffill().shift(i*7)\n",
    "        for i in range(1,1+n_dias_alante//7):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_semanas_adelante\"]=datos_var_est[var].resample('1D').ffill().shift(-i*7)\n",
    "        datos_meteo_buenos_est_list.append(datos_var_est)\n",
    "        \n",
    "    datos_var_est=pd.concat(datos_meteo_buenos_est_list,axis=1).reset_index()\n",
    "    datos_meteo_buenos_list.append(datos_var_est)\n",
    "datos_meteo_buenos=pd.concat(datos_meteo_buenos_list).dropna()\n",
    "datos_meteo_buenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_part=pd.merge(phenological_data.reset_index(), cadastral_data, left_on='codigo', right_on='codigo', how='outer', indicator=True)\n",
    "datos_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_part2=datos_part[datos_part._merge=='both']\n",
    "datos_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_campos=[]\n",
    "for campo in datos_part2.codigo.unique():\n",
    "    datos_part3=datos_part2[datos_part2.codigo==campo]\n",
    "    datos_campos.append(datos_part3.sort_values('date').set_index('date').resample('1D').ffill().dropna())\n",
    "    \n",
    "datos_part=pd.concat(datos_campos).reset_index()\n",
    "datos_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_part['anio']=datos_part['date'].dt.year\n",
    "datos_part['dia']=datos_part['date'].dt.dayofyear\n",
    "datos_part=datos_part.drop(columns=['index','_merge','date'])\n",
    "datos_part=pd.get_dummies(datos_part,columns=['variedad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_meteo_buenos['anio']=datos_meteo_buenos.fecha.dt.year\n",
    "datos_meteo_buenos['dia']=datos_meteo_buenos.fecha.dt.dayofyear\n",
    "datos_total=pd.merge(datos_part, datos_meteo_buenos, left_on=['closest','anio','dia'], right_on=['estacion','anio','dia'])\n",
    "datos_total=datos_total.drop(['closest','fecha','estacion'],axis=1)\n",
    "datos_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''select codigo, date, AVG(min) as min, AVG(max) as max, AVG(mean) as mean, AVG(std) as std, AVG(meidan) as median from \n",
    "public.copernicus_nvdi where pixels_array is not null and tesela is not null\n",
    "group by codigo, date;'''\n",
    "satelital_data = pd.read_sql_query(query, con=conexion).drop_duplicates()\n",
    "conexion.commit()\n",
    "satelital_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdatas_list=[]\n",
    "for campo in satelital_data.codigo.unique():\n",
    "    subdata=satelital_data[satelital_data.codigo==campo]\n",
    "    subdata['date']=pd.to_datetime(subdata['date'],format='%Y-%m-%d')\n",
    "    subdata['date2']=subdata['date']\n",
    "    subdata=subdata.sort_values('date').set_index('date').resample('1D').ffill().reset_index()\n",
    "    subdata['diff']=(subdata['date']-subdata['date2']).dt.days\n",
    "    subdatas_list.append(subdata[subdata['diff']<21].drop(columns=['diff','date2']))\n",
    "satelital_data2=pd.concat(subdatas_list)\n",
    "satelital_data2['dia']=satelital_data2.date.dt.dayofyear\n",
    "satelital_data2['anio']=satelital_data2.date.dt.year\n",
    "satelital_data2=satelital_data2.drop(columns=['date'])\n",
    "satelital_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conexion.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_total2=pd.merge(datos_total, satelital_data2, left_on=['codigo','anio','dia'], right_on=['codigo','anio','dia'])\n",
    "# datos_total2=datos_total2.drop(columns=['codigo'])\n",
    "datos_total2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_total2.columns[400:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial,train,vali,estado_fen):\n",
    "    variedades=[col for col in train.columns if 'variedad_' in col]\n",
    "    variables_basic=['phenologystageid','dia', 'min', 'max', 'mean', 'std', 'median']+variedades\n",
    "    medidas=variables_basic\n",
    "    \n",
    "    altitud=trial.suggest_categorical('Altitud',[True,False])\n",
    "    latitud=trial.suggest_categorical('Latitud',[True,False])\n",
    "    longitud=trial.suggest_categorical('Longitud',[True,False])\n",
    "    if(longitud):\n",
    "        medidas.append('longitude')\n",
    "    if(latitud):\n",
    "        medidas.append('latitude')\n",
    "    if(altitud):\n",
    "        medidas.append('altitud')\n",
    "        \n",
    "    if (trial.suggest_categorical('Acumuladas',[True,False])):\n",
    "        \n",
    "        inicio=trial.suggest_categorical('Inicio mediciones',['t0','1','2'])\n",
    "        suffix=''\n",
    "        name_vars=[]\n",
    "        \n",
    "        chilling=trial.suggest_categorical('Chilling',[True,False])\n",
    "        winkler=trial.suggest_categorical('Winkler',[True,False])\n",
    "        gdd=trial.suggest_categorical('gdd',[True,False])\n",
    "        \n",
    "        acumulativo=trial.suggest_categorical('Acumulativo',[True,False])\n",
    "        if (acumulativo):\n",
    "            suffix='_Cumm'\n",
    "            if(trial.suggest_categorical('Precipitaciones',[True,False])):\n",
    "                name_vars.append('precip__'+inicio+'_')\n",
    "            if(trial.suggest_categorical('Radiacion',[True,False])):\n",
    "                print('rad__'+inicio+'_')\n",
    "                name_vars.append('rad__'+inicio+'_')\n",
    "                \n",
    "        if (winkler|gdd):\n",
    "            tbase=trial.suggest_categorical('Tbase',['Tbase','TbaseMax'])\n",
    "            temperatura_inicio=trial.suggest_categorical('Temperatura inicio',['10.0','4.5'])\n",
    "        if (chilling):\n",
    "            tbase_chill=trial.suggest_categorical('Tbase_chilling',['Tbase','Tbasemin','Utah'])\n",
    "            name_vars.append('chillingDD_7.0_'+inicio+'_' + tbase_chill +'_sum')\n",
    "        if (winkler):\n",
    "            if acumulativo:\n",
    "                name_vars.append('winkler_'+temperatura_inicio+'_'+inicio+'_' + tbase)\n",
    "            else:\n",
    "                name_vars.append('winkler_'+temperatura_inicio+'_' + tbase)\n",
    "        if (gdd):\n",
    "            name_vars.append('gdd_'+temperatura_inicio+'_'+inicio+'_' + tbase+'_sum')\n",
    "       \n",
    "        if len(name_vars)>0:\n",
    "            semanas_list=[]\n",
    "            for i in range(1,3):\n",
    "                if(trial.suggest_categorical(str(i)+'_semanas_atras',[True,False])):\n",
    "                    semanas_list.append(' '+str(i)+'_semanas_atras')\n",
    "            if(trial.suggest_categorical('1_semanas_adelante',[True,False])):\n",
    "                    semanas_list.append(' 1_semanas_adelante')\n",
    "                    \n",
    "            for name_var in name_vars:\n",
    "                medidas.append(name_var+suffix)\n",
    "                for semana in semanas_list:\n",
    "                    medidas.append(name_var+suffix+semana)\n",
    "        \n",
    "    else:\n",
    "        name_vars=[]    \n",
    "        if(trial.suggest_categorical('Temperatura',[True,False])):\n",
    "            if(trial.suggest_categorical('Temperatura_media',[True,False])):\n",
    "                name_vars.append('tmed_mean')\n",
    "            if(trial.suggest_categorical('Temperatura_min',[True,False])):\n",
    "                name_vars.append('tmed_min')\n",
    "            if(trial.suggest_categorical('Temperatura_max',[True,False])):\n",
    "                name_vars.append('tmed_max')\n",
    "\n",
    "        if(trial.suggest_categorical('Radiacion',[True,False])):\n",
    "            if(trial.suggest_categorical('Radiacion_media',[True,False])):\n",
    "                name_vars.append('rad_mean')\n",
    "            if(trial.suggest_categorical('Radiacion_min',[True,False])):\n",
    "                name_vars.append('rad_min')\n",
    "            if(trial.suggest_categorical('Radiacion_max',[True,False])):\n",
    "                name_vars.append('rad_max')\n",
    "\n",
    "        if(trial.suggest_categorical('Viento',[True,False])):\n",
    "            if(trial.suggest_categorical('Viento_norte',[True,False])):\n",
    "                name_vars.append('wind_N')\n",
    "            if(trial.suggest_categorical('Viento_noreste',[True,False])):\n",
    "                name_vars.append('wind_NE')\n",
    "            if(trial.suggest_categorical('Viento_este',[True,False])):\n",
    "                name_vars.append('wind_E')\n",
    "            if(trial.suggest_categorical('Viento_sureste',[True,False])):\n",
    "                name_vars.append('wind_SE')\n",
    "            if(trial.suggest_categorical('Viento_sur',[True,False])):\n",
    "                name_vars.append('wind_S')\n",
    "            if(trial.suggest_categorical('Viento_suroeste',[True,False])):\n",
    "                name_vars.append('wind_SW')\n",
    "            if(trial.suggest_categorical('Viento_oeste',[True,False])):\n",
    "                name_vars.append('wind_W')\n",
    "            if(trial.suggest_categorical('Viento_noroeste',[True,False])):\n",
    "                name_vars.append('wind_NW')\n",
    "        \n",
    "        if len(name_vars)>0:\n",
    "            dias_vars=[]\n",
    "            for i in range(1,15):\n",
    "                if(trial.suggest_categorical(str(i)+'_dias_atras',[True,False])):\n",
    "                    dias_vars.append(' ' + str(i) + '_dias_atras')\n",
    "            for i in range(1,8):\n",
    "                if(trial.suggest_categorical(str(i)+'_dias_adelante',[True,False])):\n",
    "                    dias_vars.append(' ' + str(i) + '_dias_adelante')\n",
    "\n",
    "            for name_var in name_vars:\n",
    "                medidas.append(name_var)\n",
    "                for dia_var in dias_vars:\n",
    "                    medidas.append(name_var+dia_var)\n",
    "\n",
    "    medidas.append('dias_hasta')\n",
    "    train=train[medidas].dropna()\n",
    "    vali=vali[medidas].dropna()\n",
    "    \n",
    "    print(medidas)\n",
    "    print(len(medidas))\n",
    "\n",
    "    X_train=train.drop(['dias_hasta'], axis=1).values\n",
    "    Y_train=train['dias_hasta'].values\n",
    "    X_vali=vali.drop(['dias_hasta'], axis=1).values\n",
    "    Y_vali=vali['dias_hasta'].values\n",
    "\n",
    "    clear_session()\n",
    "\n",
    "    inputA = Input(shape=(X_train.shape[1],), name='Entrada')\n",
    "\n",
    "    y = Dense(int(trial.suggest_discrete_uniform('Neuronas capa 0',low=16,high=512,q=16)))(inputA)\n",
    "    y = Activation(trial.suggest_categorical('Activacion capa 0',[\"selu\",\"linear\",\"tanh\",\"softmax\"]))(y)\n",
    "    \n",
    "    capas=trial.suggest_int('Numero de capas',low=3,high=8)\n",
    "    \n",
    "    for i in range(1,capas-2):\n",
    "        y = Dense(int(trial.suggest_discrete_uniform('Neuronas capa {}'.format(i),low=16,high=512,q=64)))(y)\n",
    "        y = Activation(trial.suggest_categorical('Activacion capa {}'.format(i),[\"selu\",\"linear\",\"tanh\",\"softmax\"]))(y)\n",
    "        \n",
    "    for i in range(capas-2,capas):\n",
    "        y = Dense(int(trial.suggest_discrete_uniform('Neuronas capa {}'.format(i),low=16,high=256,q=16)))(y)\n",
    "        y = Activation(trial.suggest_categorical('Activacion capa {}'.format(i),[\"selu\",\"linear\",\"tanh\",\"softmax\"]))(y)\n",
    "        \n",
    "    z = Dense(1, activation=\"selu\")(y)\n",
    "\n",
    "    model = Model(inputs=inputA, outputs=z)\n",
    "    \n",
    "    callback = [EarlyStopping(monitor='val_loss',patience=17, min_delta=1.7,restore_best_weights=False),\n",
    "            ModelCheckpoint('/data/proyectos/GRAPEVINE/Models/all_data/Intento1/Estado_feno_' + str(estado_fen) + '/checkpointed_model.h5', monitor='val_loss',\n",
    "                            save_best_only=True)]\n",
    "    \n",
    "    learning_rate=trial.suggest_categorical('Learning rate',[10**-3, 10**-2, 10**-1])\n",
    "   \n",
    "    choiceval = trial.suggest_categorical('Optimizador',['sgd','adam','rmsprop'])\n",
    "    if choiceval == 'adam':\n",
    "        optim = keras.optimizers.Adam(lr=learning_rate)\n",
    "    elif choiceval == 'rmsprop':\n",
    "        optim = keras.optimizers.RMSprop(lr=learning_rate)\n",
    "    else:\n",
    "        optim = keras.optimizers.SGD(lr=learning_rate)\n",
    "\n",
    "    weights=None\n",
    "    \n",
    "    model.save('/data/proyectos/GRAPEVINE/Models/all_data/Intento1/Estado_feno_' + str(estado_fen) + '/checkpointed_model.h5')\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=optim, metrics=['mse'])\n",
    "    model.fit(X_train, Y_train, epochs=1000, verbose=0, \n",
    "              batch_size=trial.suggest_int('Batch size',low=1,high=16), \n",
    "              callbacks=callback, validation_split=0.2, class_weight=weights)\n",
    "    \n",
    "    model=load_model('/data/proyectos/GRAPEVINE/Models/all_data/Intento1/Estado_feno_' + str(estado_fen) + '/checkpointed_model.h5')\n",
    "        \n",
    "    \n",
    "    preds = model.predict(X_vali)\n",
    "\n",
    "    SCORE=r2_score(Y_vali,preds)\n",
    "    print(\"R^2: \",SCORE)\n",
    "    \n",
    "    nombre='/data/proyectos/GRAPEVINE/Models/all_data/Intento1/Estado_feno_' + str(estado_fen) + '/model-estado'+ str(estado_fen) +'-'+str(SCORE)\n",
    "     \n",
    "    if SCORE>0.65:    \n",
    "        model_json = model.to_json()\n",
    "        with open(nombre+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        model.save_weights(nombre+\".h5\")\n",
    "    \n",
    "    print('#'*100)\n",
    "    \n",
    "    return SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_no_val=datos_total2[datos_total2.anio<=2020]\n",
    "datos_no_val.phenologystageid=datos_no_val.phenologystageid.replace({9.0:0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state  = 17\n",
    "direction     = 'maximize'\n",
    "n_trials      = 5\n",
    "n_jobs        = 1\n",
    "timeout       = None\n",
    "verbosity     = 0\n",
    "trozos=20\n",
    "\n",
    "\n",
    "for estado_fen in [3,5]:\n",
    "    temp=datos_no_val.copy()\n",
    "\n",
    "    datos_list=[]\n",
    "    for campaña in temp.anio.unique():\n",
    "        datos_camp=temp[temp['anio']==campaña]\n",
    "        for id_terr in datos_camp.codigo.unique():\n",
    "            datos_camp_terr=datos_camp[datos_camp['codigo']==id_terr]\n",
    "            x=datos_camp_terr[datos_camp_terr['phenologystageid']>=estado_fen]['dia'].values\n",
    "            momento=0\n",
    "            if len(x>0):\n",
    "                momento=np.min(x)\n",
    "            datos_camp_terr['dias_hasta']=momento-datos_camp_terr['dia']\n",
    "            datos_list.append(datos_camp_terr[datos_camp_terr['dias_hasta']>0])\n",
    "    datos_final=pd.concat(datos_list)\n",
    "    train=datos_final[datos_final.anio!=2018]\n",
    "    vali=datos_final[datos_final.anio==2018]\n",
    "    train=train.drop(columns=['codigo','anio'])\n",
    "    print(len(train))\n",
    "    vali=vali.drop(columns=['codigo','anio'])\n",
    "\n",
    "    study_name = f'Estado-'+str(estado_fen)\n",
    "    sampler    = optuna.samplers.TPESampler(seed=random_state)\n",
    "\n",
    "    FILE = f'/data/proyectos/GRAPEVINE/Models/all_data/Intento1/Estado_feno_' + str(estado_fen) + '/resumen_optuna-estado'+str(estado_fen)+'-r2-dias.csv'\n",
    "\n",
    "    study = optuna.create_study(study_name=study_name,direction=direction,sampler=sampler,\n",
    "                                storage='sqlite:////data/proyectos/GRAPEVINE/Models/all_data/Intento1/Estado_feno_' + str(estado_fen) + '/optuna-aceituna-estado'+str(estado_fen)+'-r2-dias.db', \n",
    "                                load_if_exists=True)\n",
    "\n",
    "    for j in range(trozos):\n",
    "        study.optimize(func=lambda trial: objective(trial,train,vali,estado_fen),\n",
    "                       n_trials=n_trials,timeout=timeout)\n",
    "\n",
    "        result_df = study.trials_dataframe()\n",
    "        result_df.to_csv(FILE)\n",
    "        time.sleep(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
