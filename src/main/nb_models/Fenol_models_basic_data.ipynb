{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.backend import clear_session\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import optuna\n",
    "\n",
    "import psycopg2\n",
    "from config import *\n",
    "\n",
    "import utm\n",
    "import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest(lon,lat,estaciones):\n",
    "    distancias_list=[]\n",
    "    cercano=0\n",
    "    closest=10000000\n",
    "    for i in range(len(estaciones)):\n",
    "        dist=(estaciones['longitude'].iloc[i]-lon)**2+(estaciones['latitude'].iloc[i]-lat)**2\n",
    "        distancias_list.append(dist)\n",
    "        if(dist<closest):\n",
    "            closest=dist\n",
    "            cercano=estaciones['nombrecorto'].iloc[i]\n",
    "    estaciones['distancias']=distancias_list\n",
    "    return cercano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conexion = psycopg2.connect(database=db_database, \n",
    "                                user=db_user, \n",
    "                                password=db_password, \n",
    "                                host=db_host, \n",
    "                                port=db_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''select date, variedad, min(phenologystageid) as phenologystageid, codigocatastro as codigo \n",
    "from redfara.redfara_fenologia\n",
    "where especie='VIÑEDO VINIFICACION'\n",
    "group by date, variedad, codigo, phenologystageid;'''\n",
    "phenological_data = pd.read_sql_query(query, con=conexion).drop_duplicates()\n",
    "conexion.commit()\n",
    "phenological_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='select * from cadastral.parcelas where codigo in '+ str(list(phenological_data.codigo.unique())).replace('[','(').replace(']',')') + ';'\n",
    "cadastral_data = pd.read_sql_query(query, con=conexion)\n",
    "conexion.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cadastral_data['longitude']=cadastral_data['coordenadas_epsgwgs84'].apply(lambda x:float(x[0]))\n",
    "cadastral_data['latitude']=cadastral_data['coordenadas_epsgwgs84'].apply(lambda x:float(x[1]))\n",
    "cadastral_data=cadastral_data[['codigo', 'longitude','latitude','altitud']].drop_duplicates()\n",
    "cadastral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='select * from public.estara;'\n",
    "stations = pd.read_sql_query(query, con=conexion).drop_duplicates()\n",
    "conexion.commit()\n",
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations['longitude']=stations.longitud.str.split(\"'|º\").apply(lambda x:int(x[0])+int(x[1])/60+int(x[2])/3600000)\n",
    "stations['latitude']=stations.latitud.str.split(\"'|º\").apply(lambda x:int(x[0])+int(x[1])/60+int(x[2])/3600000)\n",
    "stations=stations[['nombrecorto','longitude','latitude', 'altitud']]\n",
    "stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cercanias=[]\n",
    "for i in range(len(cadastral_data)):\n",
    "    cercanias.append(get_closest(cadastral_data.iloc[i].longitude,cadastral_data.iloc[i].latitude,stations))\n",
    "cadastral_data['closest']=cercanias\n",
    "cadastral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_stations = stations[stations.nombrecorto.isin(cadastral_data.closest.unique())]\n",
    "closest_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_year=phenological_data.date.dt.year.min()\n",
    "lowest_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='select * from public.meteorological_data WHERE anio >= ' + str(lowest_year) + ' AND estacion in ' + str(list(closest_stations.nombrecorto.unique())).replace('[','(').replace(']',')') + ';'\n",
    "meteorological_data = pd.read_sql_query(query, con=conexion).drop_duplicates()\n",
    "conexion.commit()\n",
    "meteorological_data = meteorological_data.drop(['ubi', 'season', 'hourFrac_sum'], axis=1)\n",
    "variables_diarias_min=['tmed_min', 'rad_min']\n",
    "variables_diarias_max=['tmed_max', 'rad_max']\n",
    "variables_diarias_mean=['tmed_mean', 'rad_mean', 'wind_N', 'wind_NE', 'wind_E','wind_SE', 'wind_S', 'wind_SW', \n",
    "                        'wind_W', 'wind_NW']\n",
    "variables_semanales=['gdd_4.5_t0_Tbase_sum',\n",
    "       'gdd_4.5_t0_TbaseMax_sum', 'gdd_4.5_1_Tbase_sum',\n",
    "       'gdd_4.5_1_TbaseMax_sum', 'gdd_4.5_2_Tbase_sum',\n",
    "       'gdd_4.5_2_TbaseMax_sum', 'gdd_10.0_t0_Tbase_sum',\n",
    "       'gdd_10.0_t0_TbaseMax_sum', 'gdd_10.0_1_Tbase_sum',\n",
    "       'gdd_10.0_1_TbaseMax_sum', 'gdd_10.0_2_Tbase_sum',\n",
    "       'gdd_10.0_2_TbaseMax_sum', 'chillingDD_7.0_t0_Tbase_sum',\n",
    "       'chillingDD_7.0_t0_Tbasemin_sum', 'chillingDD_7.0_t0_Utah_sum',\n",
    "       'chillingDD_7.0_1_Tbase_sum', 'chillingDD_7.0_1_Tbasemin_sum',\n",
    "       'chillingDD_7.0_1_Utah_sum', 'chillingDD_7.0_2_Tbase_sum',\n",
    "       'chillingDD_7.0_2_Tbasemin_sum', 'chillingDD_7.0_2_Utah_sum', 'rad_sum',\n",
    "       'precip_sum', 'winkler_4.5_Tbase', 'winkler_4.5_TbaseMax',\n",
    "       'winkler_10.0_Tbase', 'winkler_10.0_TbaseMax',\n",
    "       'gdd_4.5_t0_Tbase_sum_Cumm', 'gdd_4.5_t0_TbaseMax_sum_Cumm',\n",
    "       'gdd_4.5_1_Tbase_sum_Cumm', 'gdd_4.5_1_TbaseMax_sum_Cumm',\n",
    "       'gdd_4.5_2_Tbase_sum_Cumm', 'gdd_4.5_2_TbaseMax_sum_Cumm',\n",
    "       'gdd_10.0_t0_Tbase_sum_Cumm', 'gdd_10.0_t0_TbaseMax_sum_Cumm',\n",
    "       'gdd_10.0_1_Tbase_sum_Cumm', 'gdd_10.0_1_TbaseMax_sum_Cumm',\n",
    "       'gdd_10.0_2_Tbase_sum_Cumm', 'gdd_10.0_2_TbaseMax_sum_Cumm',\n",
    "       'chillingDD_7.0_t0_Tbase_sum_Cumm',\n",
    "       'chillingDD_7.0_t0_Tbasemin_sum_Cumm',\n",
    "       'chillingDD_7.0_t0_Utah_sum_Cumm', 'chillingDD_7.0_1_Tbase_sum_Cumm',\n",
    "       'chillingDD_7.0_1_Tbasemin_sum_Cumm', 'chillingDD_7.0_1_Utah_sum_Cumm',\n",
    "       'chillingDD_7.0_2_Tbase_sum_Cumm', 'chillingDD_7.0_2_Tbasemin_sum_Cumm',\n",
    "       'chillingDD_7.0_2_Utah_sum_Cumm', 'rad__t0__Cumm', 'rad__1__Cumm',\n",
    "       'rad__2__Cumm', 'precip__t0__Cumm', 'precip__1__Cumm',\n",
    "       'precip__2__Cumm', 'winkler_4.5_t0_Tbase_Cumm',\n",
    "       'winkler_4.5_t0_TbaseMax_Cumm', 'winkler_4.5_1_Tbase_Cumm',\n",
    "       'winkler_4.5_1_TbaseMax_Cumm', 'winkler_4.5_2_Tbase_Cumm',\n",
    "       'winkler_4.5_2_TbaseMax_Cumm', 'winkler_10.0_t0_Tbase_Cumm',\n",
    "       'winkler_10.0_t0_TbaseMax_Cumm', 'winkler_10.0_1_Tbase_Cumm',\n",
    "       'winkler_10.0_1_TbaseMax_Cumm', 'winkler_10.0_2_Tbase_Cumm',\n",
    "       'winkler_10.0_2_TbaseMax_Cumm']\n",
    "meteorological_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteorological_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_meteo_buenos_list=[]\n",
    "n_dias_atras=15\n",
    "n_dias_alante=8\n",
    "for estacion in meteorological_data['estacion'].unique():\n",
    "    datos_est=meteorological_data[meteorological_data.estacion==estacion].sort_values('fecha').set_index('fecha')\n",
    "    datos_meteo_buenos_est_list=[datos_est[['estacion']]]\n",
    "    for var in variables_diarias_min:\n",
    "        datos_var_est=datos_est[[var]].resample('1D').min()\n",
    "        for i in range(1,n_dias_atras):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_atras\"]=datos_var_est[var].resample('1D').min().shift(i)\n",
    "        for i in range(1,n_dias_alante):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_adelante\"]=datos_var_est[var].resample('1D').min().shift(-i)\n",
    "        datos_meteo_buenos_est_list.append(datos_var_est)\n",
    "    for var in variables_diarias_max:\n",
    "        datos_var_est=datos_est[[var]].resample('1D').max()\n",
    "        for i in range(1,n_dias_atras):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_atras\"]=datos_var_est[var].resample('1D').max().shift(i)\n",
    "        for i in range(1,n_dias_alante):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_adelante\"]=datos_var_est[var].resample('1D').max().shift(-i)\n",
    "        datos_meteo_buenos_est_list.append(datos_var_est)\n",
    "    for var in variables_diarias_mean:\n",
    "        datos_var_est=datos_est[[var]].resample('1D').max()\n",
    "        for i in range(1,n_dias_atras):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_atras\"]=datos_var_est[var].resample('1D').max().shift(i)\n",
    "        for i in range(1,n_dias_alante):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_dias_adelante\"]=datos_var_est[var].resample('1D').max().shift(-i)\n",
    "        datos_meteo_buenos_est_list.append(datos_var_est)\n",
    "    for var in variables_semanales:\n",
    "        datos_var_est=datos_est[[var]].resample('1D').max()\n",
    "        for i in range(1,1+n_dias_atras//7):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_semanas_atras\"]=datos_var_est[var].resample('1D').ffill().shift(i*7)\n",
    "        for i in range(1,1+n_dias_alante//7):\n",
    "            datos_var_est[var + \" \" + str(i) + \"_semanas_adelante\"]=datos_var_est[var].resample('1D').ffill().shift(-i*7)\n",
    "        datos_meteo_buenos_est_list.append(datos_var_est)\n",
    "        \n",
    "    datos_var_est=pd.concat(datos_meteo_buenos_est_list,axis=1).reset_index()\n",
    "    datos_meteo_buenos_list.append(datos_var_est)\n",
    "datos_meteo_buenos=pd.concat(datos_meteo_buenos_list).dropna()\n",
    "datos_meteo_buenos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_part=pd.merge(phenological_data.reset_index(), cadastral_data, left_on='codigo', right_on='codigo', how='outer', indicator=True)\n",
    "datos_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_part2=datos_part[datos_part._merge=='both']\n",
    "datos_part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_campos=[]\n",
    "for campo in datos_part2.codigo.unique():\n",
    "    datos_part3=datos_part2[datos_part2.codigo==campo]\n",
    "    datos_campos.append(datos_part3.sort_values('date').set_index('date').resample('1D').ffill().dropna())\n",
    "    \n",
    "datos_part=pd.concat(datos_campos).reset_index()\n",
    "datos_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_part['anio']=datos_part['date'].dt.year\n",
    "datos_part['dia']=datos_part['date'].dt.dayofyear\n",
    "datos_part=datos_part.drop(columns=['index','_merge','date'])\n",
    "datos_part=pd.get_dummies(datos_part,columns=['variedad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_meteo_buenos['anio']=datos_meteo_buenos.fecha.dt.year\n",
    "datos_meteo_buenos['dia']=datos_meteo_buenos.fecha.dt.dayofyear\n",
    "datos_total=pd.merge(datos_part, datos_meteo_buenos, left_on=['closest','anio','dia'], right_on=['estacion','anio','dia'])\n",
    "datos_total=datos_total.drop(['closest','fecha','estacion'],axis=1)\n",
    "datos_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''select codigo, date, AVG(min) as min, AVG(max) as max, AVG(mean) as mean, AVG(std) as std, AVG(meidan) as median from \n",
    "public.copernicus_nvdi where pixels_array is not null and tesela is not null\n",
    "group by codigo, date;'''\n",
    "satelital_data = pd.read_sql_query(query, con=conexion).drop_duplicates()\n",
    "conexion.commit()\n",
    "satelital_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdatas_list=[]\n",
    "for campo in satelital_data.codigo.unique():\n",
    "    subdata=satelital_data[satelital_data.codigo==campo]\n",
    "    subdata['date']=pd.to_datetime(subdata['date'],format='%Y-%m-%d')\n",
    "    subdata['date2']=subdata['date']\n",
    "    subdata=subdata.sort_values('date').set_index('date').resample('1D').ffill().reset_index()\n",
    "    subdata['diff']=(subdata['date']-subdata['date2']).dt.days\n",
    "    subdatas_list.append(subdata[subdata['diff']<21].drop(columns=['diff','date2']))\n",
    "satelital_data2=pd.concat(subdatas_list)\n",
    "satelital_data2['dia']=satelital_data2.date.dt.dayofyear\n",
    "satelital_data2['anio']=satelital_data2.date.dt.year\n",
    "satelital_data2=satelital_data2.drop(columns=['date'])\n",
    "satelital_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conexion.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_total2=pd.merge(datos_total, satelital_data2, left_on=['codigo','anio','dia'], right_on=['codigo','anio','dia'])\n",
    "# datos_total2=datos_total2.drop(columns=['codigo'])\n",
    "datos_total2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_total2.columns[400:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial,train,vali,estado_fen):\n",
    "    \n",
    "    medidas=['phenologystageid','dia']\n",
    "    if(trial.suggest_categorical('Variedades',[True,False])):\n",
    "        variedades=[col for col in train.columns if 'variedad_' in col]\n",
    "        medidas=medidas+variedades\n",
    "    \n",
    "    altitud=trial.suggest_categorical('Altitud',[True,False])\n",
    "    latitud=trial.suggest_categorical('Latitud',[True,False])\n",
    "    longitud=trial.suggest_categorical('Longitud',[True,False])\n",
    "    if(longitud):\n",
    "        medidas.append('longitude')\n",
    "    if(latitud):\n",
    "        medidas.append('latitude')\n",
    "    if(altitud):\n",
    "        medidas.append('altitud')\n",
    "\n",
    "    medidas.append('dias_hasta')\n",
    "    train=train[medidas].dropna()\n",
    "    vali=vali[medidas].dropna()\n",
    "\n",
    "    X_train=train.drop(['dias_hasta'], axis=1).values\n",
    "    Y_train=train['dias_hasta'].values\n",
    "    X_vali=vali.drop(['dias_hasta'], axis=1).values\n",
    "    Y_vali=vali['dias_hasta'].values\n",
    "\n",
    "    clear_session()\n",
    "\n",
    "    inputA = Input(shape=(X_train.shape[1],), name='Entrada')\n",
    "\n",
    "    y = Dense(int(trial.suggest_discrete_uniform('Neuronas capa 0',low=16,high=464,q=16)))(inputA)\n",
    "    y = Activation(trial.suggest_categorical('Activacion capa 0',[\"selu\",\"linear\",\"tanh\",\"softmax\"]))(y)\n",
    "    \n",
    "    capas=trial.suggest_int('Numero de capas',low=3,high=8)\n",
    "    \n",
    "    for i in range(1,capas-2):\n",
    "        y = Dense(int(trial.suggest_discrete_uniform('Neuronas capa {}'.format(i),low=16,high=512,q=64)))(y)\n",
    "        y = Activation(trial.suggest_categorical('Activacion capa {}'.format(i),[\"selu\",\"linear\",\"tanh\",\"softmax\"]))(y)\n",
    "        \n",
    "    for i in range(capas-2,capas):\n",
    "        y = Dense(int(trial.suggest_discrete_uniform('Neuronas capa {}'.format(i),low=16,high=256,q=16)))(y)\n",
    "        y = Activation(trial.suggest_categorical('Activacion capa {}'.format(i),[\"selu\",\"linear\",\"tanh\",\"softmax\"]))(y)\n",
    "        \n",
    "    z = Dense(1, activation=\"selu\")(y)\n",
    "\n",
    "    model = Model(inputs=inputA, outputs=z)\n",
    "    \n",
    "    callback = [EarlyStopping(monitor='val_loss',patience=17, min_delta=1.7,restore_best_weights=False),\n",
    "            ModelCheckpoint('/data/proyectos/GRAPEVINE/Models/all_data/Intento1/Estado_feno_' + str(estado_fen) + '/checkpointed_model.h5', monitor='val_loss',\n",
    "                            save_best_only=True)]\n",
    "    \n",
    "    learning_rate=trial.suggest_categorical('Learning rate',[10**-3, 10**-2, 10**-1])\n",
    "   \n",
    "    choiceval = trial.suggest_categorical('Optimizador',['sgd','adam','rmsprop'])\n",
    "    if choiceval == 'adam':\n",
    "        optim = keras.optimizers.Adam(lr=learning_rate)\n",
    "    elif choiceval == 'rmsprop':\n",
    "        optim = keras.optimizers.RMSprop(lr=learning_rate)\n",
    "    else:\n",
    "        optim = keras.optimizers.SGD(lr=learning_rate)\n",
    "\n",
    "    sample_weights=((Y_train<=20)&(Y_train>=5)).astype(int)*10+((Y_train<=40)).astype(int)*6+1\n",
    "    \n",
    "    model.save('/data/proyectos/GRAPEVINE/Models/basic_data/Intento1/Estado_feno_' + str(estado_fen) + '/checkpointed_model.h5')\n",
    "    \n",
    "    model.compile(loss='mse', optimizer=optim, metrics=['mse'])\n",
    "    batch_power=trial.suggest_int('Batch power',low=1,high=10)\n",
    "    model.fit(X_train, Y_train, epochs=1000, verbose=0, \n",
    "              batch_size=2**batch_power, \n",
    "              callbacks=callback, validation_split=0.2, sample_weight=sample_weights)\n",
    "    \n",
    "    model=load_model('/data/proyectos/GRAPEVINE/Models/basic_data/Intento1/Estado_feno_' + str(estado_fen) + '/checkpointed_model.h5')\n",
    "        \n",
    "    \n",
    "    preds = model.predict(X_vali)\n",
    "\n",
    "    sample_weights2=((Y_vali<=21)&(Y_vali>=7)).astype(int)*10+((Y_vali<=60)).astype(int)*6+1\n",
    "    SCORE=r2_score(Y_vali,preds, sample_weight=sample_weights2)\n",
    "    \n",
    "    print(\"R^2: \",SCORE)\n",
    "    \n",
    "    nombre='/data/proyectos/GRAPEVINE/Models/basic_data/Intento1/Estado_feno_' + str(estado_fen) + '/model-estado'+ str(estado_fen) +'-'+str(SCORE)\n",
    "     \n",
    "    if SCORE>0.65:    \n",
    "        model_json = model.to_json()\n",
    "        with open(nombre+\".json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        model.save_weights(nombre+\".h5\")\n",
    "    \n",
    "    print('#'*100)\n",
    "    \n",
    "    return SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_no_val=datos_total2[datos_total2.anio<=2020]\n",
    "datos_no_val.phenologystageid=datos_no_val.phenologystageid.replace({9.0:0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state  = 17\n",
    "direction     = 'maximize'\n",
    "n_trials      = 5\n",
    "n_jobs        = 1\n",
    "timeout       = None\n",
    "verbosity     = 0\n",
    "trozos=20\n",
    "\n",
    "\n",
    "for estado_fen in range(1,9):\n",
    "    temp=datos_no_val.copy()\n",
    "\n",
    "    datos_list=[]\n",
    "    for campaña in temp.anio.unique():\n",
    "        datos_camp=temp[temp['anio']==campaña]\n",
    "        for id_terr in datos_camp.codigo.unique():\n",
    "            datos_camp_terr=datos_camp[datos_camp['codigo']==id_terr]\n",
    "            x=datos_camp_terr[datos_camp_terr['phenologystageid']>=estado_fen]['dia'].values\n",
    "            momento=0\n",
    "            if len(x>0):\n",
    "                momento=np.min(x)\n",
    "            datos_camp_terr['dias_hasta']=momento-datos_camp_terr['dia']\n",
    "            datos_list.append(datos_camp_terr[datos_camp_terr['dias_hasta']>0])\n",
    "    datos_final=pd.concat(datos_list)\n",
    "    train=datos_final[datos_final.anio!=2018]\n",
    "    vali=datos_final[datos_final.anio==2018]\n",
    "    train=train.drop(columns=['codigo','anio'])\n",
    "    print(len(train))\n",
    "    vali=vali .drop(columns=['codigo','anio'])\n",
    "\n",
    "    study_name = f'Estado-'+str(estado_fen)\n",
    "    sampler    = optuna.samplers.TPESampler(seed=random_state)\n",
    "\n",
    "    FILE = f'/data/proyectos/GRAPEVINE/Models/basic_data/Intento1/Estado_feno_' + str(estado_fen) + '/resumen_optuna-estado'+str(estado_fen)+'-r2-dias.csv'\n",
    "\n",
    "    study = optuna.create_study(study_name=study_name,direction=direction,sampler=sampler,\n",
    "                                storage='sqlite:////data/proyectos/GRAPEVINE/Models/basic_data/Intento1/Estado_feno_' + str(estado_fen) + '/optuna-aceituna-estado'+str(estado_fen)+'-r2-dias.db', \n",
    "                                load_if_exists=True)\n",
    "\n",
    "    for j in range(trozos):\n",
    "        study.optimize(func=lambda trial: objective(trial,train,vali,estado_fen),\n",
    "                       n_trials=n_trials,timeout=timeout)\n",
    "\n",
    "        result_df = study.trials_dataframe()\n",
    "        result_df.to_csv(FILE)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
